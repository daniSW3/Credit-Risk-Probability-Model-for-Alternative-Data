{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ced5dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (95662, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>BatchId</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>SubscriptionId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>ProviderId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>ChannelId</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Value</th>\n",
       "      <th>TransactionStartTime</th>\n",
       "      <th>PricingStrategy</th>\n",
       "      <th>FraudResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionId_76871</td>\n",
       "      <td>BatchId_36123</td>\n",
       "      <td>AccountId_3957</td>\n",
       "      <td>SubscriptionId_887</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_10</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>2018-11-15T02:18:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TransactionId_73770</td>\n",
       "      <td>BatchId_15642</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-11-15T02:19:08Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionId_26203</td>\n",
       "      <td>BatchId_53941</td>\n",
       "      <td>AccountId_4229</td>\n",
       "      <td>SubscriptionId_222</td>\n",
       "      <td>CustomerId_4683</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_1</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>2018-11-15T02:44:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionId_380</td>\n",
       "      <td>BatchId_102363</td>\n",
       "      <td>AccountId_648</td>\n",
       "      <td>SubscriptionId_2185</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_1</td>\n",
       "      <td>ProductId_21</td>\n",
       "      <td>utility_bill</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>21800</td>\n",
       "      <td>2018-11-15T03:32:55Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TransactionId_28195</td>\n",
       "      <td>BatchId_38780</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-644.0</td>\n",
       "      <td>644</td>\n",
       "      <td>2018-11-15T03:34:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
       "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
       "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
       "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
       "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
       "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
       "\n",
       "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
       "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
       "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
       "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
       "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
       "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
       "\n",
       "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
       "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
       "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
       "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
       "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
       "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
       "\n",
       "   PricingStrategy  FraudResult  \n",
       "0                2            0  \n",
       "1                2            0  \n",
       "2                2            0  \n",
       "3                2            0  \n",
       "4                2            0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering Pipeline (Jupyter Notebook Version)\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    FunctionTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from xverse.transformer import MonotonicBinning\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data\n",
    "data_path = r\"C:\\Users\\Daniel.Temesgen\\Desktop\\KIAM-Rsc\\week5\\Data\\data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic info\n",
    "print(\"Data shape:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8149e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom transformers (same as in .py file)\n",
    "\n",
    "class FeatureAggregator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create aggregate features from transaction data\"\"\"\n",
    "    def __init__(self, customer_id_col='customer_id', amount_col='amount'):\n",
    "        self.customer_id_col = customer_id_col\n",
    "        self.amount_col = amount_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Make a copy to avoid SettingWithCopyWarning\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Group by customer and create aggregate features\n",
    "        agg_features = X.groupby(self.customer_id_col)[self.amount_col].agg([\n",
    "            ('total_amount', 'sum'),\n",
    "            ('avg_amount', 'mean'),\n",
    "            ('transaction_count', 'count'),\n",
    "            ('amount_std', 'std'),\n",
    "            ('amount_min', 'min'),\n",
    "            ('amount_max', 'max')\n",
    "        ]).reset_index()\n",
    "        \n",
    "        # Fill NA for std (which occurs when only 1 transaction exists)\n",
    "        agg_features['amount_std'] = agg_features['amount_std'].fillna(0)\n",
    "        \n",
    "        # Merge back with original data\n",
    "        X = X.merge(agg_features, on=self.customer_id_col, how='left')\n",
    "        \n",
    "        return X\n",
    "\n",
    "class DateTimeExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from datetime columns\"\"\"\n",
    "    def __init__(self, datetime_col='transaction_date'):\n",
    "        self.datetime_col = datetime_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if self.datetime_col in X.columns:\n",
    "            X[self.datetime_col] = pd.to_datetime(X[self.datetime_col])\n",
    "            X['transaction_hour'] = X[self.datetime_col].dt.hour\n",
    "            X['transaction_day'] = X[self.datetime_col].dt.day\n",
    "            X['transaction_month'] = X[self.datetime_col].dt.month\n",
    "            X['transaction_year'] = X[self.datetime_col].dt.year\n",
    "            X['day_of_week'] = X[self.datetime_col].dt.dayofweek\n",
    "            \n",
    "        return X\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handle categorical variable encoding\"\"\"\n",
    "    def __init__(self, one_hot_cols=None, label_encode_cols=None):\n",
    "        self.one_hot_cols = one_hot_cols or []\n",
    "        self.label_encode_cols = label_encode_cols or []\n",
    "        self.one_hot_encoder = None\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.one_hot_cols:\n",
    "            self.one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "            self.one_hot_encoder.fit(X[self.one_hot_cols])\n",
    "            \n",
    "        if self.label_encode_cols:\n",
    "            for col in self.label_encode_cols:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(X[col])\n",
    "                self.label_encoders[col] = le\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # One-hot encoding\n",
    "        if self.one_hot_cols and self.one_hot_encoder:\n",
    "            one_hot_features = self.one_hot_encoder.transform(X[self.one_hot_cols])\n",
    "            one_hot_df = pd.DataFrame(\n",
    "                one_hot_features,\n",
    "                columns=self.one_hot_encoder.get_feature_names_out(self.one_hot_cols))\n",
    "            X = pd.concat([X.drop(self.one_hot_cols, axis=1), one_hot_df], axis=1)\n",
    "            \n",
    "        # Label encoding\n",
    "        if self.label_encode_cols:\n",
    "            for col in self.label_encode_cols:\n",
    "                if col in X.columns:  # Check if column wasn't dropped during one-hot\n",
    "                    X[col] = self.label_encoders[col].transform(X[col])\n",
    "                    \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eef046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'Amount', 'Value', 'TransactionStartTime', 'PricingStrategy', 'FraudResult']\n"
     ]
    }
   ],
   "source": [
    "# Add this at the beginning of your notebook or script after loading the data\n",
    "print(\"Columns in the dataset:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fac1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using last column 'FraudResult' as target variable\n"
     ]
    }
   ],
   "source": [
    "# Find which column is likely your target (modify as needed)\n",
    "# For credit risk models, common target names include: 'default', 'target', 'label', 'bad_flag', etc.\n",
    "target_col = 'default'  # Change this to your actual target column name\n",
    "\n",
    "# Or you can try to automatically detect a binary target column\n",
    "possible_targets = ['default', 'target', 'label', 'bad_flag', 'is_bad', 'fraud']\n",
    "for col in possible_targets:\n",
    "    if col in data.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "else:\n",
    "    # If none of the common names exist, use the last column as target (common in some datasets)\n",
    "    target_col = data.columns[-1]\n",
    "    print(f\"Using last column '{target_col}' as target variable\")\n",
    "\n",
    "# Now proceed with the split\n",
    "X = data.drop(target_col, axis=1)\n",
    "y = data[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c47514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically detected target column: FraudResult\n"
     ]
    }
   ],
   "source": [
    "# Define column types (modify according to your actual data)\n",
    "numerical_cols = ['amount', 'total_amount', 'avg_amount', 'transaction_count', 'amount_std']\n",
    "categorical_ohe_cols = ['product_category', 'transaction_type']\n",
    "categorical_le_cols = ['country']\n",
    "datetime_col = 'transaction_date'\n",
    "\n",
    "# Automatically detect or specify target column\n",
    "target_col = 'default'  # Change this to your actual target column name\n",
    "if target_col not in data.columns:\n",
    "    # Try to find a binary column that might be the target\n",
    "    binary_cols = [col for col in data.columns if data[col].nunique() == 2]\n",
    "    if binary_cols:\n",
    "        target_col = binary_cols[0]\n",
    "        print(f\"Automatically detected target column: {target_col}\")\n",
    "    else:\n",
    "        # Last resort - use last column\n",
    "        target_col = data.columns[-1]\n",
    "        print(f\"Using last column '{target_col}' as target variable\")\n",
    "\n",
    "# Split data\n",
    "X = data.drop(target_col, axis=1)\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4688fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'Amount', 'Value', 'TransactionStartTime', 'PricingStrategy', 'FraudResult']\n",
      "\n",
      "Sample data:\n",
      "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
      "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
      "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
      "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
      "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
      "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
      "\n",
      "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
      "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
      "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
      "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
      "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
      "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
      "\n",
      "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
      "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
      "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
      "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
      "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
      "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
      "\n",
      "   PricingStrategy  FraudResult  \n",
      "0                2            0  \n",
      "1                2            0  \n",
      "2                2            0  \n",
      "3                2            0  \n",
      "4                2            0  \n",
      "\n",
      "Processing completed successfully!\n",
      "Training data shape: (76529, 18)\n",
      "Test data shape: (19133, 18)\n",
      "\n",
      "Processed columns: ['Amount', 'Value', 'CurrencyCode_UGX', 'CountryCode_256', 'ProductCategory_airtime', 'ProductCategory_data_bundles', 'ProductCategory_financial_services', 'ProductCategory_movies', 'ProductCategory_other', 'ProductCategory_ticket', 'ProductCategory_transport', 'ProductCategory_tv', 'ProductCategory_utility_bill', 'PricingStrategy_0', 'PricingStrategy_1', 'PricingStrategy_2', 'PricingStrategy_4', 'target']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "data_path = r\"C:\\Users\\Daniel.Temesgen\\Desktop\\KIAM-Rsc\\week5\\Data\\data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Print column information for verification\n",
    "print(\"Columns in dataset:\", data.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "print(data.head())\n",
    "\n",
    "# ======================================================================\n",
    "# STEP 1: Define your column mappings based on actual data\n",
    "# ======================================================================\n",
    "\n",
    "# Map to our expected column types:\n",
    "TARGET_COL = 'FraudResult'              # Your binary target variable\n",
    "DATETIME_COL = 'TransactionStartTime'   # Your timestamp column\n",
    "AMOUNT_COL = 'Amount'                   # Your transaction amount column\n",
    "CUSTOMER_ID_COL = 'CustomerId'          # Your customer identifier\n",
    "\n",
    "# Define which features to use for different transformations\n",
    "NUMERICAL_COLS = ['Amount', 'Value']    # Numerical features to scale\n",
    "CATEGORICAL_OHE_COLS = [                # Categorical features for one-hot encoding\n",
    "    'CurrencyCode', \n",
    "    'CountryCode',\n",
    "    'ProductCategory',\n",
    "    'PricingStrategy'\n",
    "]\n",
    "\n",
    "# ======================================================================\n",
    "# STEP 2: Define custom transformers (updated)\n",
    "# ======================================================================\n",
    "\n",
    "class FeatureAggregator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create aggregate features per customer\"\"\"\n",
    "    def __init__(self, customer_id_col='CustomerId', amount_col='Amount'):\n",
    "        self.customer_id_col = customer_id_col\n",
    "        self.amount_col = amount_col\n",
    "        self.agg_feature_names = [\n",
    "            'total_amount', 'avg_amount', 'transaction_count',\n",
    "            'amount_std', 'amount_min', 'amount_max'\n",
    "        ]\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if self.customer_id_col in X.columns and self.amount_col in X.columns:\n",
    "            agg_features = X.groupby(self.customer_id_col)[self.amount_col].agg([\n",
    "                ('total_amount', 'sum'),\n",
    "                ('avg_amount', 'mean'),\n",
    "                ('transaction_count', 'count'),\n",
    "                ('amount_std', 'std'),\n",
    "                ('amount_min', 'min'),\n",
    "                ('amount_max', 'max')\n",
    "            ]).reset_index()\n",
    "            \n",
    "            agg_features['amount_std'] = agg_features['amount_std'].fillna(0)\n",
    "            X = X.merge(agg_features, on=self.customer_id_col, how='left')\n",
    "        \n",
    "        return X\n",
    "\n",
    "class DateTimeExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from datetime column\"\"\"\n",
    "    def __init__(self, datetime_col='TransactionStartTime'):\n",
    "        self.datetime_col = datetime_col\n",
    "        self.new_features = [\n",
    "            'transaction_hour', 'transaction_day',\n",
    "            'transaction_month', 'transaction_year',\n",
    "            'day_of_week'\n",
    "        ]\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if self.datetime_col in X.columns:\n",
    "            X[self.datetime_col] = pd.to_datetime(X[self.datetime_col])\n",
    "            X['transaction_hour'] = X[self.datetime_col].dt.hour\n",
    "            X['transaction_day'] = X[self.datetime_col].dt.day\n",
    "            X['transaction_month'] = X[self.datetime_col].dt.month\n",
    "            X['transaction_year'] = X[self.datetime_col].dt.year\n",
    "            X['day_of_week'] = X[self.datetime_col].dt.dayofweek\n",
    "        \n",
    "        return X\n",
    "\n",
    "# ======================================================================\n",
    "# STEP 3: Create the processing pipeline (updated)\n",
    "# ======================================================================\n",
    "\n",
    "def create_feature_pipeline():\n",
    "    \"\"\"Create the complete feature engineering pipeline\"\"\"\n",
    "    \n",
    "    # Numerical features processing\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Categorical features processing\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Combine all transformers\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numerical_transformer, NUMERICAL_COLS),\n",
    "        ('cat', categorical_transformer, CATEGORICAL_OHE_COLS)\n",
    "    ])\n",
    "    \n",
    "    # Main pipeline with all steps\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('datetime_extractor', DateTimeExtractor(datetime_col=DATETIME_COL)),\n",
    "        ('feature_aggregator', FeatureAggregator(\n",
    "            customer_id_col=CUSTOMER_ID_COL, \n",
    "            amount_col=AMOUNT_COL\n",
    "        )),\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# ======================================================================\n",
    "# STEP 4: Process the data (updated)\n",
    "# ======================================================================\n",
    "\n",
    "if TARGET_COL in data.columns:\n",
    "    X = data.drop(TARGET_COL, axis=1)\n",
    "    y = data[TARGET_COL]\n",
    "    \n",
    "    # Split into train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create and fit pipeline\n",
    "    pipeline = create_feature_pipeline()\n",
    "    X_train_processed = pipeline.fit_transform(X_train, y_train)\n",
    "    X_test_processed = pipeline.transform(X_test)\n",
    "    \n",
    "    # Get the correct feature names from the pipeline\n",
    "    def get_feature_names(pipeline):\n",
    "        \"\"\"Extract feature names from pipeline components\"\"\"\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get numerical feature names\n",
    "        if 'num' in pipeline.named_steps['preprocessor'].named_transformers_:\n",
    "            feature_names.extend(NUMERICAL_COLS)\n",
    "        \n",
    "        # Get one-hot encoded feature names\n",
    "        if 'cat' in pipeline.named_steps['preprocessor'].named_transformers_:\n",
    "            ohe = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "            ohe_features = ohe.get_feature_names_out(CATEGORICAL_OHE_COLS)\n",
    "            feature_names.extend(ohe_features)\n",
    "        \n",
    "        return feature_names\n",
    "    \n",
    "    feature_names = get_feature_names(pipeline)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    if X_train_processed.shape[1] == len(feature_names):\n",
    "        X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "        X_test_df = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "    else:\n",
    "        print(f\"Warning: Shape mismatch ({X_train_processed.shape[1]} features, {len(feature_names)} names)\")\n",
    "        print(\"Using default column names\")\n",
    "        X_train_df = pd.DataFrame(X_train_processed)\n",
    "        X_test_df = pd.DataFrame(X_test_processed)\n",
    "    \n",
    "    # Add target back to DataFrames\n",
    "    X_train_df['target'] = y_train.reset_index(drop=True)\n",
    "    X_test_df['target'] = y_test.reset_index(drop=True)\n",
    "    \n",
    "    # Save processed data\n",
    "    X_train_df.to_csv('train_processed.csv', index=False)\n",
    "    X_test_df.to_csv('test_processed.csv', index=False)\n",
    "    \n",
    "    print(\"\\nProcessing completed successfully!\")\n",
    "    print(f\"Training data shape: {X_train_df.shape}\")\n",
    "    print(f\"Test data shape: {X_test_df.shape}\")\n",
    "    print(\"\\nProcessed columns:\", X_train_df.columns.tolist())\n",
    "else:\n",
    "    print(f\"Error: Target column '{TARGET_COL}' not found in data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
